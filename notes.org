* Thompson et al, 2011

[[https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0018093][Link to the paper]] This paper is a benchmark study of multiple sequence alignment
methods. Modern methods are pretty good, but face some challenges: (1) locally
conserved regions are less well aligned, (2) motifs in natively disordered
regions are often misaligned, and (3) the badly predicted or fragmentary protien
sequences in many of today's databases lead to significant number of alignment
errors. Novel approaches will be required to deal with the most difficult
regions.

** Introduction

*** Definitions 

MSA obviously.

BAliBASE -- this is an alignment benchmark suite containing multiple sequence
alignments organized into 9 reference sets representing specific MSA problems.
[Q: what does alignment benchmark suit mean?] The different reference sets
represent specific problems for MSA problems. These include (1) small numbers of
sequences, (2) unequal phylogenetic distributions, (3) large N/C-terminal
extensions or internal insertions, (4) repeats, inverted domains and
transmembrane regions.

Specific features are known as *specificity determining positions* (SPDs) that
modulate a protien's function in a given context.

The most popular alignment algorithms traditionally have been *progressive
alignment procedures* which exploits the fact that progressive alignments are
evolutionarily related. An MSA is built up gradually using a series of pairwise
alignments, following the branching order in a phylogenetic tree. [Q: do we need
a phylogenetic tree to do this?]

A *global MSA algorithm* attempts to align the full length sequences one end to
the other. Often we start with a global alignment and then focus on important
regions. A *local algorithm* attempts to identify subsequences sharing high
similarity. Then the unreliable or low-similarity regions are either excluded or
differentiated (eg by the use of upper/lower case letters).

Gold standard benchmarks indicate that no existing progressive algorithm is
capable of providing accurate alignments for all test cases. As a result,
iterative algorithms have been developed, e.g. Hidden Markov Models and Genetic
Algorithms. These are good for highly conserved regions, but bad for distantly
related sequences.

Challenges: (1) Scalability is a big challenge because we can now make MSAs with
hundreds of organisms; (2) small number of model organisms in public databases;
(3) protein families represented in today's databases are complicated, and (4)
new sequences are mostly predicted by automatic methods and thus contain a large
number of sequence errors [Q: this has something to do about gene sequence
prediction? See bottom of p2]; (5) we now have sequences with billions of bases
produced from read lengths as short as 35-45 nucleotides, resulting in
fragmentary protein sequences. In short, the data is big and noisy.

*** Purpose of this paper

The purpose is to apply a new protein sequence alignment benchmark designed to
give a comprehensive account of the performance of today's most popular MSA
programs. This was motivated by two observations: (1) most of the existing MSA
benchmarks have focused on the patterns conserved in hte majority of the
sequences and not enough attention has been paid to the less-frequent patterns,
or SDPs which might indicate subfamily- or context-specific functions; (2)
current MSA programs for protein sequences generally model globular domain
structure and evolution. [Q: what is "globular domain structure"?] But many
protiens in eukaryotes are unstructured, or contain large unstructured regions
[Q: what does sctructured mean in this context?]

Specifically, the benchmark represents 218 large and complex protein families
and has been incorporated into the BAliBASE benchmark suite. Basically, they
include a new test set, Reference 10, which they add to the pre-existing 9
reference sets in BAliBASE. The new reference set focuses on subamily specific
features, motifs in disordered regions, and the effect of fragmentary or
erroneous sequences on MSA quality. [Q: what does reference set refer to in this
context?]

This is a comparative study to evaulate the most widely-used alignment methods.

[RANDOM QUESTION: Strong selection for gene 1 in the past should speed up
coalescent rate for unrelated gene 2, no? That is, it should have an effect
similar to reducing the effective population size.]



** Results

*** Overall Alignment Quality

Applied 8 alignment programs to each of 218 reference alignments in the
benchmark, resulting in 1744 automatically-constructed MSAs.

Overall quality was measured with Column Score (CS). [what is this? ask methods
team]

Probcons, TCoffee and Mafft did the best. (Scores about 80%). Mafft took only an
hours but the other ones took like 3 days. Kalign took only 3 minutes but had
some loss of accuracy (74%).

Methods that did not combine both local and global approach performed worse.

*** Effect of sequence discrepancies ons alignment quality

Methods did well. But I don't understand what is meant by sequence discrepencies. 

*Discrepencies* are unexpected or discordant extensions, inserions or deletions,
as shown in Figure 2.

*** MSA program evaluation: alignment of locally conserved motifs

Want to see if the MSAs can identify context-specific or locally conserved
motifs.

Various features of blocks in the reference alingments were measured, such as
length of hte block, sequence similarity, frequency the bock is show in the
alignment, and what percent of the block is found in a disordered region.
Alignment quality for each block was measured with a scoring system called BCS.
Programs did not respond in the same way to different block features. For
example, Muscle and TCoffee were affected by the frequency of the blocks.
Probcons and Muscle were less sensitive to similarity of the blocks.

Blocks with significant number of residues in "natively disordered segments" had
low scores with every program. [what does that mean?]

What is a disordered segment?

*** Improving local alignment quality by combining methods

They construct a theoretical top score by choosing for each block the top score
achieved by any of the inference methods considered.

* Di Franco et al. 2019
[[https:bmcecolevol.biomedcentral.com/track/pdf/10.1186/s12862-019-1350-2.pdf][Link to the paper]]. 
** Issue
Molecular evolutionary analysis begins with the construction of multiple
sequence alignments. It is therefore important to limit the propagation of
alignment errors. Potential errors are multiple: alignment errors, as well as
the more ominous "primary errors" which include sequencing errors, assembly
errors. and annotation errors.
** Significance
** Approach
The authors develop an HMM called HmmCleaner, which detects and eliminates
errors in MSAs. They perform two analyses:

  1. They first assess the performance of HmmCleaner using ~700 amino-acid MSAs
     in which primary (i.e. non-alignment) errors were artificially introduced.

  2. Next they compared two "segment-filtering methods", HmmCleaner and PREQUAL,
     with block-filtering software (BMGE and TrimAI). Specifically, they
     compared the impact on evolutionary analysis using vertebrate data.
** Methods
- HmmCleaner has four steps: 
  1. create a profile pHMM based on the MSA
  2. estimate the probability that the pHMM generates each amino acid of a given
     sequence of the MSA (the assumption here is that a primary sequence error
     should have very low probability)
  3. calculate a cumulative similarity score for each sequence
  4. segments with score zero or lower are "low similarity segments"
- Dataset creation:
  1. MSAs of protein-coding genes from prokaryotes. Purpose: to study
     performance of HMMCleaner.
  2. Datasets assembled from animal sequences. Purpose: to study impact on
     evolutionary inferences.
- Simulator:
  - Input: a protein-coding alignment (MSA)
  - Randomly introduces primary sequence errors (e.g. scrambling a segment or
    inserting a scrambled segment, frameshift (i.e. the insertion/deletion of a
    number of sites not divisible by three))
  - Translates all sequences to protiens and realigns them with MAFFT
  - Output: an MSA with errors
  - Then run HmmCleaner on the output MSA. Regions with low similarity scores
    are compared to the locations of the simulated errors to obtain type I and
    type II error probabilities.
- Parameter Optimization
  - HMMCleaner has 4 scoring parameters. HMMCleaner was run on the prokaryotic
    datasets using thousands of combinations of parameters in order to find the
    best values to use.
- Characterization of HmmCleaner performance:
  - Prokaryotic Data
  - HmmCleaner was compared to 4 other filtering methods
- Effect of HmmCleaner on evolutionary inferences
  - Animal MSAs
  - Selection: For each simulation, we test for positive simulation using (1) the original
    MSA (2) the MSA cleaned by the programs, (3) erroneous MSA, and (4) the
    erroneous filtered MSA.
  - Phylogenies: Tested 11 different filtering setups, then used RAxML on each MSA
** Results
In the first analysis, HmmCleaner exhibited >95% sensitivity to primary sequence
errors. HmmmCleaner targets low-similarity segments for removal, rather than whole blocks.

In the second analysis, they find that segment-filtering methods improve
evolutionary inference better than block-filtering, especially with respect to
improving branch length inferences and reducing false positive rate during
detection of positive selection.
** Conclusions
- Reccommend the use of segment-filtering methods for eukaryotic genomic data.
- Primary sequence errors more detrimental than alignment errors (didn't understand why)
- 
** Questions
Does anyone really know how illumina works? Don't they use proprietary methods?
Like... doesn't that preclude replication?
* Ballesteros & Hormiga 2016
[[https://doi.org/10.1093/molbev/msw069][link to paper]] this paper introduces a phylogenetic orthology method that does
not depend on root placement and does not require explicit assignment of groups.
this method is applied to a small insects dataset. also apply to rnaseq data
from spiders--to identify and document stable orthologs.

** issue
a fundamental assumption in phylogenetics is that the loci used to infer
evolutionary relationships are orthologs. most methods of orthology detection
evaluate orthology using some measurement of similarity. most methods don't
perform better than blast. in addition, orthology detection usually requires
computationally intensive pairwaise comparisons (for distance-based methods) or
tree inference (for phylogeny based methods).

tree-based orthology detection methods are increasingly popular. but inferred
orthologies may depend on the root of the tree.

there are not enough stable orthology hypotheses, especially for invertabrates.


rna sequencing (aka "transcriptomics") is increasingly used for phylogenetics.
but while these efforts usually involve some form of orthology and homology
assessment, but since the main goal is to recover the species tree, the
orthology detection results are generally treated as byproducts and are not
saved for future reference.

** definitions
"tree-based orthology detection methods" are methods that use the phylogeny of
gene families to infer events of duplication-speciation of the gene copies,

"stable orthology hypotheses" = a database of identified "ultraconserved
elements" (uces) which can be individually accessed and further documented.

** significance
not using orthologs can lead to phylogenetic conflict.
** approach
introduce upho (unrooted phylogenetic orthology), an orthology assessment method
which does not depend on the root in the input gene family trees. then
 - test upho on a trivial example of vertebrate hemoglobins.
 - using a small insect dataset, compare the use of upho with other methods when
   used in a phylogenomic pipeline.
 - test upho on transcriptomic data using a dataset of 27 spider species. the
   goal was to identify orthologs at "specific hierarchical levels." the spider
   dataset was evaluated under a variety of homology (clustering) methods.

** methods
upho is an orthology assessment tool that is used after homology detection step
and construction of gene trees.
** results
a gene family tree is a green trees whose tips reside in the leaves of the
species tree. not all leaves of the species tree contain tips of the gene tree;
and some leaves may have more.

the method upho classifies splits in gene trees by comparing the number of leafs of
the gene tree with the number of species tree leaves that it is contained in. if
these numbers are the same, then the split is thought to be orthologous.
otherwise it is a "paralogous split" (i.e. assumed to arise from a duplication).
if the tips of the gene tree are all contained in a single leaf of the species
tree, then the split is called "in-paralogous".

*vertebrate hemoglobins:* didn't really understand this.

*insects proteomes:* a clustering algorithm identified 2538 clusters (i.e.
potential orthologs) from 1906 alignments. the results were input to upho and
ptp (some other method: phylotreepruner) and were compared with results from a
dataset called oma.

upho consistently discovered more orthogroups than ptp and oma.

astral consistently recovered the species phylogeny regardless of orthology
assessment. [this does not surprise me given our consistency results in the
dlcoal project.]

*spiders:* blast produced 12,000 clusters with at least 5 species. when
accepting in-paralogs, they discovered 3x as many orthologs as ptp. sc datasets
showed heterogenous and conflicted phylogenetic signals. see figure 6 for a nice
picture of the resulting tree.

** conclusions
** questions
what exactly does "tree pruning" mean? 

what is ribosomal dna? 

"have a restricted set of molecular markers available for phylogenetic
inference" -- what does this mean? not much dna?

what does blast do? blast finds regions of similarity between biological
sequences. it compares sequences to sequence databases and calculates statistica
significance.

what is ancestor-descendant polarity?

what is proteome? all the proteins in a cell

what does it mean for a cluster to be composed of "single copy" (sc) genes?
* Yang 2017
** problem 3: fair-balance paradox. the true model is standard normal. we compare
$h_1$ and $h_2$ which are normal mean $\mu$ but with variances bigger or less
than 1, chosen so that their kl divergence from the truth is equidistant
("equally wrong"). uniform prior over the models (each pr 1/2). p_1 is the
posterior model probabilities.

as n goes to infinity, p_1 congerges to a 2-point distribution at 0 and 1, each
with probability 1/2. when one is closer to the truth, convergence to the
correct model occurs, but the more wrong model can have high posterior for
finite n (eg 99% in 5% of datasets when n=100). this is a "twilight zone" in
which there is high probability of rejecting the true tree.

** star-tree paradox and bayesian phylogenetics
3 simple cases of trees with just 3 or 4 species. 
*** case a: equally right models
t_0 is the star tree with branch length t=0.2 the best-fitting parameter values
are internal branch length 0 and other branch length 0.2. the prior
probabilities on the three binary trees is that they are equally likely. jc
substitution model is used to both generate and analyze the data.

*** case b: equally wrong models
jc+gamma model of evolution is used to generate the data. but the data are
analyzed using jc. in this cases the three binary trees are equally wrong
because under each binary tree, the mle for the branch lengths is t_0 = 0 and
t_1 = 0.164. the kl divergence is positive and equal for each of the binary
trees.

*** case c: equally wrong and distinct models
simulation model is jc+gamma, but analysis model is jc. molecular clock is not
assumed. we consider unrooted four-leaf trees. for all of the three binary
trees, the best-fitting parameter values are are (internal) t_0=0.01 and
t_i=0.164 for i=1,2,3,4 (pendant edges). the posterior degenerates into a
probability measure that assigns 100$ to exactly one of the binary trees (which
happens to each tree with probability 1/3). this is "type-3 polarized behavior."
this is the most important scenario for real phylogenetic analysis.

*** discussion
**** high posterior probabilities for phylogenetic trees
this work was motivated by the problem of spuriously high posterior
probabilities for phylogenetic trees. one explanation for this is the failure of
current evolutionary models to accommodate interdependence among sites in the
sequence, which leads to exaggeration of the amount of information in the data.
but consideration of coding/noncoding genes suggesets this might not be the true
cause of the issue.

the problem might be deeper than that. it may be a consequence of the polarized
nature of bayesian model selection when all models are misspecified. 

**** bayesian selection of opposing misspecified models
this paper considered the asymptotic behavior of the bayesian method as data
size goes to infinity, but in the cases where there are model selection
problems. "equally right" and "equally wrong" cases were considered.

the authors categorize three different types of asymptotic behavior:

- type-1: ("sensible") -- the posterior probability converges to a sensible
  value, such as 1/2. unfortunately, this occurs only when the two models are
  identical or overlapping.
- type-2: ("volatile") -- the posterior distribution fluctates among datasets
  like a random number, so that some models may have support. this occurs when
  models are equally right or equally wrong but "indistinct". less of a problem,
  since this occurs only when models are essentially the same interpretation of
  the data. for example, if the truth is a star tree and the posterior gives
  strong support to a random one of three binary trees, all of which have the
  same, very short internal branch length (and hence should be suported as
  evidence of polytomy).
- type-3: ("polarize") -- the posterior is approximately zero in half the
  datasets and approximately 1 in the other half. this occurs when two models
  are equally wrong and distinct. most relevant to practical application,
  because all models are wrong. when one model is slightly less wrong, it will
  win with infinite data, but bayesian inference will be overconfident and
  support the more wrong model with high posterior too often.

something about "posterior predictive distribution".

extreme sensitivity fo the assumed model is undesirable. there are two heuristic
approaches to remedy the high posterior model probabilities in the context of
phylogenies:

1. assing nonzero probabilities to multifurcating trees (e.g. star trees) in the
   prior.
2. let internal branch length in the binary trees become increasingly smaller in
   the prior when the data size increases.

**** non-bayesian methods
cox's likelihood-ratio test--which conducts multiple tests with each model used
as the null--can be used to compare nonnested models. not widely used in
phylogentics for computational reasons (too many trees).

the bootstrap support is the most commonly used method for attaching a measure
of confidence in the maximum-likelihood tree. for this, sites (alignment
columns) are sampled to generate n bootstrap pseudodatasets and then we
calculate the bootstrap support value for a clade (a node on the tree) to be the
the proportion of times (out of n) in which that node is inferred in the ml
tree. a straightforward interpretation of the bootstrap support values for trees
remains elusive.

*** my questions:
i would like to better understand this sentence:

"this application of bootstrap for model comparison appears to have important
differences from the conventional bootstrap for calculating the standard errors
and confidence intervals for a parameter estimate (40); a straightforward inter-
pretation of the bootstrap support values for trees remains elusive (31,
41–43)."
* Tiley et al 2020 
[[https://reader.elsevier.com/reader/sd/pii/S0168952520301311?token=C3BDBB19B4575B231CC92FE6B6233468D2CCC5A709D321F7799820A8ADA1AFF3DAC16E620612F8E5C71C3ED9203392C3&originRegion=us-east-1&originCreation=20220302202938][Link to Paper]] "Molecular Clocks without Rocks"
** Issue
The assumption of a strict molecular clock is unreasonable if species are not
closely related. For example, great apes have lower substitution rates than
monkeys--largely because great apes live longer and reproduce more slowly than
monkeys (i.e. 1 great ape generation = x years > y years = 1 monkey generation time)
** Definitions
A *Pedigree trio* is a child and two parents for whom whole genomes are
sequenced and compared to identify the new mutations in the child. 

The *de novo mutation rate* is the spontaneous germline mutation rate.

A *coalescent age estimate* is the divergence time for two sequences based on
sampling theory and measured in the expected number of generations.

A *callable site* is a site where a de novo mutation should be detectable.
Typically needs to be estimated.

*Pairwise sequence divergence:* evolutionary distance between a pair of
 sequences measured as the expected number of substitutions per site.

** Significance
Relaxation of molecular clock assumptions may improve power of divergence time
estimation.

This is important because estimates of divergence times impact our
interpretations of trait evolution, biogeography, and processes that underly
species radiations.

** Approach
Compare (i) concatentation and (ii) MSC that accounts for ILS. Both methods can
be used without fossil calibrations provided that one has apriori info about
mutation rates.
*** Discrepancies between Concatenation and MSC Methods for Divergence Time Estimates
The divergence time of human and chimp is estimated as:
  - 5.7-10 MYA using fossil-calibrated concatenation and MSC methods
  - 8.2 MYA using mutation-rate calibrated MSC
  - 12.1 MYA assuming human mutation rate using pairwise sequence distances (but
    subtracing effective population size of human-chimp common ancestor gives
    7.9 MYA)
Takeaway: both concatenation and MSC approaches can obtain estimates of
divergence time without fossile evidence. However, concatenation requires an
estimate of ancestral effective population size.

These methods are sensitive to mutation rate estimates. For example, estimating
human-monkey divergence time using human mutation rates for both humans and
monkeys gives 62 MYA, which is much greater than fossil-based estimates of 35
MYA. This is likely due to slower mutation rate in humans than in monkeys.

An empirical study with mouse lemurs found MSC methods yield significantly more
recent age estimates than fossil-calibrated concatenation methdod (1.5 MYA vs 10
MYA). There are many possible reasons for this
  - overestimation of mutation rate estimate
  - bad fossil calibrations (due to being based on phylogenetically-distant taxa)
  - tendency for fossil-calibrated concatenation to overestimate young nodes
    (due to use of older fossil calibrations)
Takeaway: many complex factors can contribute to differences between
divergence-time estimates obtained from MSC methods vs concatenation.

*** A New Frontier in Divergence Time Estimation
Evaluation of MSC methods for divergence time estimation is in its infancy.
Therefore, additional empirical evidence is needed to fully understand the
effect of ILS on divergence time estimation. Future studies should compare
traditional clock model estimates with MSC estimates.

When converting coalescent units into units of absolute times, uncertainty in
mutation rates and generation times should be taken explicitly into account
(e.g. by drawing these from prior distributions rather than relying on point
estimates).

*** Concluding Remarks
MSC method is more computationally intensive than concatenation. 

Therefore, traditional clock analyses that use concatenation may be more
practical for large phylogenies. 

Traditional clock analyses using concatenation are likely to be okay when
divergence times are old and ILS is low.

More generally, MSC methods should be preferred because ILS is prevalent in the
tree of life.

Need to understand better when these two types of methods agree or disagree.

* Salichos 2014
[[https://watermark.silverchair.com/msu061.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAugwggLkBgkqhkiG9w0BBwagggLVMIIC0QIBADCCAsoGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMaSQCCt-u6w5NHrIuAgEQgIICmzmk2Pa03FzEnYvAEzABPjc6OzHC0k1jBw3JqR9Lk0unBwEULwCM1sjvL98HerVAlG-T510znfnyxuGohidztfiE-OUtCezDMynnsXa92O6XIPkYYRogHr8EN59lkew-9fDt0b4P2maCTHsmqWlxy4uP3LLLOIYsHqGQFKXlakMFyIjSQqEr9i1QUZ6vcjSuuv2StdU9Rli_w4fq_14PN_k_F4zMmh1cNBDPyr3q4EmjYyLClXKNMV3VldFZhTTEVgTD2CXrd5gaxHyFlXIJBaPCKaLKE3IytjbmCIbtkx3C2QOj6U0c1-bWshhJgkHTtR0eN79iAqKN_amxpzMV81Cg0GAI9-Fu70w2_lmdprmngyCnsgC6RgeZN3vc0NCk1pB43QfJipqhg_ewDz_tSOZwzg4XOGvoO-CCzKyd1hnTwtmxB65jQIFIp6TvOhkzZ9eaG5MtWcgBQPLa46z8ml0nmEUF-cpNs7kYwtIWUdqGa_V82cevJpIoWLxT6EUjMHavReGt4_hkOeR3rbNxwUXO-SFFusmmu_6AX4cUoh3bpvKCYHcxxi4UlWCr5LjvpUm9octIk8uj_bLAPMjrkIUwilbJwhLIPDePrWXY3YpN_yQUZcMEHIzcxsT2Po86SYGLb_GiE4Ysthbb5jf4F2G_ZChMAn76c4uQzElV4Czw6oKvCv3A-Oeltz52XfoUL7v7RzJRs6U3Yu_RZ7aL7OBC18oRgq-xEY9pffzbF0xz8dvWB3qEqN3JopDJlmfIOPPQorFuy7-UIbq8su1lf7nvUQ9PSfhfKYJEHTLMBlsQrB-4bseJN_LjYll_hyZBXUs4I0OtNFENja8Pdf7TIZ5qtbmNIhTxSnPBkJ7QDXS0Qpk4qfWnkXM7gpQ][Link to paper]]
Title: Novel Information Theory based measures for quantifying incongruence
among phylogenetic trees.

** Defintions
IC internode certainty (an internode is an edge). IC is a function of (1) the
frequency of bipartition defined by the edge in a given set of trees and (2) the
frequency of conflicting bipartitions in the same tree set.

ICA: IC all. Similar. ("conjunction" vs "jointly")

TC: tree certaintly and TCA tree certainty all. These are calculated by summing
the values of IC or ICA over all internodes of a phylogeny.

Note: IC, ICA, TC, TCA can be calculated from many different types of data that
contain nontrivial bipartitions. For example bootstrap replicate trees.

IC and ICA measure the incongruence of a particular edge. TC and TCA measure
global incongruence between trees in the set.

All these quantities are implemented in RAxML.


IC is based on the two most prevalent conflicting bipartitions.
ICA is based on all most prevalent conflicting bipartitions. 

In a phylogenetic tree, there is a 1-1 correspondence between edges and
bipartitions of leaves:
edges <---> bipartitions of the leaves


Partitions A=W|X and B=Y|Z are *compatible* iff at least one of hte
intersections of the following four intersections of bipartition pairs is empty:
WX, WZ, XY, XZ. Otherwise they are incongruent. See figure 1.

*Shannon's entropy:* a measure of uncertainty in a random variable, defined as
#+BEGIN_SRC LaTeX
  For an r.v. $X$ with potential outcomes $X_1,\ldots,X_n$, the entropy of $X$ is
  defined as
  \begin{equation*}
  H(X)= - \sum_{n=1}^n P(X_n) \log P(X_n)
  \end{equation*}
#+END_SRC

For example, a fair coin has entropy 1. A lotto ticket (with outcomes win/lose
the lottery, with probability 1/300000000) has entropy 1/10000000.
Interpretation: there is less uncertainty about outcome of the lotto ticket than
about the coin flip.

Applying this to phylogenetics: Let $X_1,X_2$ be the frequency of support for
the two most prevalent conflicting bipartitions. Define the random variable $X$
by $X=X_i$ with probability $X_i/(X_1+X_2)$. Then *internode uncertainty* is
defined as $H(X)$, and we define IC as $IC= 1 - (internode uncertainty)$. [Note
there is a typo: they say log(n) but it should be 1.] In some cases need to flip
the sign.

IC values close to one indicate absence of conflict whereas IC values close to 0
indicate equal support for both bipartitions


Define ICA = log(n) - H(Y)

where $Y$ is a random variable defined by $Y=X_i$ with propbability
$X_i/(X_1+\ldots+X_n$, where $X_1,\ldots,X_n$ are the conflicting bipartitions
with support above some threshold (e.g. 5%).


** Issue
Issue: gene trees are often not topologically congruent either with each other
or with species phylogeny. Therefore we want to measure and quantify
incongruence that is reasonably general, not tied to particular optimality
criterion of clade support measure.

Consensus methods are one way to deal with incongruent gene trees. For example
majority-rule consensus. Consensus trees do not give information about the
distribution of conflicting bipartitions, which may cause substantial loss of
information (e.g. is the second most supported bipartition supported by 5% of
trees or 49% of trees?).

* Naser-Khdour et al 2020
Title: Assessing Confidence in Root Placement on Phylogenies: An Empirical Study
Using Non-Reversible Models for Mammals
[[https://doi.org/10.1101/2020.07.31.230144][Link to paper]]

Methods/Discussion

Issue: time reversible markov models lack the ability to infer the root
placement on an estimated phylogeny. This paper is about nonreversible models to
root phylogenies. It introduces a new bootstrap measure, the "rootstrap" which
gives information on the statistical support for any given root position.


** Notes
- Introduced rootstrap - a measure of support for root placement which does not
  depend on rooting method
- Applied rootstrap to empirical amino and nucleotide datasets inferred using
  IQ-TREE with nonreversible models
- Rootstrap is interpreted differently depending on whether ML or Bayesian setting
- RS is not a measure of accuracy of root placement; a higher rootstrap support
  indicates less variance among sites in the signal for theplacement of the
  root.
- Rootstrap gives information about the robustness of root inference with
  regards to resampling the data
- Introduced two other methods: rBED (root branch-length error distance) and
  rSED (root split error distance). These require the true position of the root
  to be known (or assumed)
- Assessed the utility of nonreversible models to root phylogenetic trees in an
  ML framework
- mammal dataset, since roots are thought to be known with some confidence for
  mammals
- specifically focused on chiroptera (bats) and cetartiodactyla (family including
  hippos, deer, dolphins, and more). In neither case could a hypothesis be
  rejected (with amino dataset), though rootstrap tended to favor one or the
  other. Opposite for nucleotide dataset.
- nonreversible amino models are more useful for inferring root position than
  nonreversible DNA models
- BIC scores indicate reversible model is better for nucleotide dataset,
  possibly due to limitations of how they inferred the NR-DNA model
- Amino acid nonreversible is surprisingly accurate for inferring root placement
  even without additional information -- identified all the roots correctly and
  with high rootstrap support. Even with small datasets, such as 50 loci.
- Our results show that the root placement was inferred correctly with high
  rootstrap support in 12 out of the 13 data sets in which the nonreversible
  model was preferable.
- Not a silver bullet
